{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413687bb-411c-41a4-8e19-cb4152a35631",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопросы:\n",
      "   q_id                                              query\n",
      "0     1                                        Номер счета\n",
      "1     2                              Где узнать бик и счёт\n",
      "2     3  Мне не приходят коды для подтверждения данной ...\n",
      "3     4  Оформила рассрочку ,но уведомлений никаких не ...\n",
      "4     5  Здравствуйте, когда смогу пользоваться кредитн...\n",
      "\n",
      "База знаний (сайты):\n",
      "   web_id                                   url  kind  \\\n",
      "0       1                  https://alfabank.ru/  html   \n",
      "1       2           https://alfabank.ru/a-club/  html   \n",
      "2       3  https://alfabank.ru/a-club/ultimate/  html   \n",
      "3       4    https://alfabank.ru/actions/rules/  html   \n",
      "4       5       https://alfabank.ru/alfafuture/  html   \n",
      "\n",
      "                                               title  \\\n",
      "0  Альфа-Банк - кредитные и дебетовые карты, кред...   \n",
      "1                      А-Клуб. Деньги имеют значение   \n",
      "2                      А-Клуб. Деньги имеют значение   \n",
      "3                                   Скидки по картам   \n",
      "4  Альфа‑Будущее: Платформа для развития студенто...   \n",
      "\n",
      "                                                text  \n",
      "0  Рассчитайте выгоду\\nРасчёт калькулятора предва...  \n",
      "1  Брокерские услуги\\nОткрытие брокерского счёта ...  \n",
      "2  Хотите получить больше информации?\\nПозвоните ...  \n",
      "3  Правила проведения Акции «Альфа Пятница. Бараб...  \n",
      "4  Образование\\nМагистратуры\\nМагистратура ВШЭ\\nМ...  \n",
      "\n",
      "--- РЕЖИM: ФИНАЛЬНАЯ ОТПРАВКА (RUN_VALIDATION = False) ---\n",
      "Вопросов для обработки (full submission): 6977\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# --- ГЛАВНЫЙ ПЕРЕКЛЮЧАТЕЛЬ ---\n",
    "# True  = Разделить данные, посчитать Hit@5 (для разработки)\n",
    "# False = Использовать ВСЕ вопросы, создать финальный submit.csv (для платформы)\n",
    "RUN_VALIDATION = False\n",
    "# ---\n",
    "\n",
    "# --- Загрузка данных ---\n",
    "try:\n",
    "    questions = pd.read_csv('questions_clean.csv')\n",
    "    websites = pd.read_csv('websites.csv')\n",
    "    sample_submission = pd.read_csv('sample_submission.csv')\n",
    "    print(\"Вопросы:\")\n",
    "    print(questions.head())\n",
    "    print(\"\\nБаза знаний (сайты):\")\n",
    "    print(websites.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Ошибка: Убедитесь, что файлы .csv находятся в той же папке.\")\n",
    "\n",
    "questions['query'] = questions['query'].fillna('')\n",
    "\n",
    "# --- Переменные для данных ---\n",
    "questions_to_process = None # Вопросы, которые пойдут в submission.csv\n",
    "questions_to_validate = None # Вопросы для подсчета Hit@5\n",
    "ground_truth_val = None      # \"Правильные\" ответы для Hit@5\n",
    "\n",
    "if RUN_VALIDATION:\n",
    "    print(\"\\n--- РЕЖИM: ВАЛИДАЦИЯ (RUN_VALIDATION = True) ---\")\n",
    "    try:\n",
    "        # ЗАМЕНИТЕ 'train.csv' НА ВАШ ФАЙЛ С РАЗМЕТКОЙ (q_id, web_id)\n",
    "        ground_truth_df = pd.read_csv('train.csv') \n",
    "        \n",
    "        all_q_ids = ground_truth_df['q_id'].unique()\n",
    "        \n",
    "        # Делим q_id\n",
    "        train_q_ids, val_q_ids = train_test_split(\n",
    "            all_q_ids, \n",
    "            test_size=400, # 400 вопросов на валидацию\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Это вопросы, которые пойдут в submission.csv\n",
    "        questions_to_process = questions[questions['q_id'].isin(train_q_ids)]\n",
    "        # Это вопросы, на которых считаем Hit@5\n",
    "        questions_to_validate = questions[questions['q_id'].isin(val_q_ids)]\n",
    "        # Это \"правильные\" ответы для Hit@5\n",
    "        ground_truth_val = ground_truth_df[ground_truth_df['q_id'].isin(val_q_ids)]\n",
    "\n",
    "        print(f\"Вопросов для обработки (submission): {len(questions_to_process)}\")\n",
    "        print(f\"Вопросов для валидации (Hit@5): {len(questions_to_validate)}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nОШИБКА: Файл 'train.csv' (с правильными ответами) не найден.\")\n",
    "        print(\"Не могу запустить режим валидации. Установите RUN_VALIDATION = False\")\n",
    "        # В реальном коде здесь лучше остановить выполнение\n",
    "        raise Exception(\"Файл 'train.csv' не найден для режима валидации.\")\n",
    "else:\n",
    "    print(\"\\n--- РЕЖИM: ФИНАЛЬНАЯ ОТПРАВКА (RUN_VALIDATION = False) ---\")\n",
    "    # Обрабатываем ВСЕ вопросы для финального submission.csv\n",
    "    questions_to_process = questions \n",
    "    questions_to_validate = None\n",
    "    ground_truth_val = None\n",
    "    \n",
    "    print(f\"Вопросов для обработки (full submission): {len(questions_to_process)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155ac073-5aa1-4613-9300-d4ab557d17d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем процесс чанкования...\n",
      "Готово. Получили 9902 чанков из 1937 документов.\n"
     ]
    }
   ],
   "source": [
    "# Этот код вставляем после загрузки websites.csv\n",
    "\n",
    "# -- Начало нового блока (Чанкование) --\n",
    "\n",
    "def get_chunks(text, chunk_size=256, overlap=64):\n",
    "    \"\"\"Простая функция для нарезки текста на слова.\"\"\"\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        chunks.append(\" \".join(chunk_words))\n",
    "    return chunks\n",
    "\n",
    "chunk_data = [] # Это будет наша новая \"база знаний\"\n",
    "\n",
    "print(\"Начинаем процесс чанкования...\")\n",
    "\n",
    "# Итерируемся по каждой строке в websites\n",
    "for index, row in websites.iterrows():\n",
    "    web_id = row['web_id']\n",
    "    title = row['title'] if pd.notna(row['title']) else ''\n",
    "    text = row['text'] if pd.notna(row['text']) else ''\n",
    "    \n",
    "    text_with_title = f\"Заголовок: {title}\\nТекст: {text}\"\n",
    "    \n",
    "    chunks = get_chunks(text_with_title, chunk_size=256, overlap=64)\n",
    "    \n",
    "    for chunk_text in chunks:\n",
    "        chunk_data.append({\n",
    "            'web_id': web_id,  # Сохраняем, какому 'web_id' принадлежит чанк\n",
    "            'text': chunk_text\n",
    "        })\n",
    "\n",
    "print(f\"Готово. Получили {len(chunk_data)} чанков из {len(websites)} документов.\")\n",
    "\n",
    "chunks_df = pd.DataFrame(chunk_data)\n",
    "\n",
    "# -- Конец нового блока --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ddc7b0-2469-472b-9921-73fb026f6c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем Cross-encoder...\n",
      "Cross-encoder загружен.\n",
      "Начинаем векторизацию корпуса (чанков)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6b9749e15b4877a06ce2d3dc0776f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма эмбеддингов корпуса (чанков): (9902, 384)\n",
      "Начинаем векторизацию вопросов для ОБРАБОТКИ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1c53d3a13c41b5957779a82b4a9cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма эмбеддингов для ОБРАБОТКИ: (6977, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2') \n",
    "print(\"Загружаем Cross-encoder...\")\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "print(\"Cross-encoder загружен.\")\n",
    "\n",
    "# Векторизация корпуса (чанков) - остается без изменений\n",
    "print(\"Начинаем векторизацию корпуса (чанков)...\")\n",
    "corpus_embeddings = model.encode(\n",
    "    chunks_df['text'].tolist(), \n",
    "    show_progress_bar=True\n",
    ")\n",
    "print(f\"Форма эмбеддингов корпуса (чанков): {corpus_embeddings.shape}\")\n",
    "\n",
    "\n",
    "# --- ИЗМЕНЕНИЯ ЗДЕСЬ ---\n",
    "# Векторизация вопросов для ОБРАБОТКИ (train или full)\n",
    "print(\"Начинаем векторизацию вопросов для ОБРАБОТКИ...\")\n",
    "query_embeddings_process = model.encode(\n",
    "    questions_to_process['query'].tolist(), \n",
    "    show_progress_bar=True\n",
    ")\n",
    "print(f\"Форма эмбеддингов для ОБРАБОТКИ: {query_embeddings_process.shape}\")\n",
    "\n",
    "# Векторизация ВАЛИДАЦИОННЫХ вопросов (если нужно)\n",
    "query_embeddings_val = None\n",
    "if RUN_VALIDATION and questions_to_validate is not None:\n",
    "    print(\"Начинаем векторизацию ВАЛИДАЦИОННЫХ вопросов...\")\n",
    "    query_embeddings_val = model.encode(\n",
    "        questions_to_validate['query'].tolist(), \n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    print(f\"Форма эмбеддингов ВАЛИДАЦИОННЫХ вопросов: {query_embeddings_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c3558f0-c7cc-4719-ad53-5ea0ed15601d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Индекс создан. Всего векторов в базе (чанков): 9902\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Нормализация корпуса\n",
    "corpus_embeddings_norm = corpus_embeddings.astype('float32')\n",
    "faiss.normalize_L2(corpus_embeddings_norm)\n",
    "\n",
    "# --- ИЗМЕНЕНИЯ ЗДЕСЬ ---\n",
    "# Нормализация вопросов для ОБРАБОТКИ\n",
    "query_embeddings_process_norm = query_embeddings_process.astype('float32')\n",
    "faiss.normalize_L2(query_embeddings_process_norm)\n",
    "\n",
    "# Нормализация ВАЛИДАЦИОННЫХ вопросов (если нужно)\n",
    "query_embeddings_val_norm = None\n",
    "if RUN_VALIDATION and query_embeddings_val is not None:\n",
    "    query_embeddings_val_norm = query_embeddings_val.astype('float32')\n",
    "    faiss.normalize_L2(query_embeddings_val_norm)\n",
    "# ---\n",
    "\n",
    "# Создание индекса (остается)\n",
    "d = corpus_embeddings_norm.shape[1]\n",
    "index = faiss.IndexFlatIP(d)    \n",
    "index.add(corpus_embeddings_norm)\n",
    "\n",
    "print(f\"Индекс создан. Всего векторов в базе (чанков): {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c1781-bb6b-4c0c-941f-abf60816a205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Начинаем поиск топ-10 кандидатов для 6977 вопросов (submission)...\n",
      "Форма массива индексов: (6977, 10)\n",
      "Начинаем Переранжировку и Агрегацию (для submission.csv)...\n"
     ]
    }
   ],
   "source": [
    "# --- Этот блок создает submission.csv из questions_to_process ---\n",
    "k_candidates = 10\n",
    "\n",
    "print(f\"\\nНачинаем поиск топ-{k_candidates} кандидатов для {len(questions_to_process)} вопросов (submission)...\")\n",
    "\n",
    "# --- ИЗМЕНЕНИЯ ЗДЕСЬ ---\n",
    "# Ищем только по эмбеддингам 'to_process'\n",
    "D, I = index.search(query_embeddings_process_norm, k_candidates) \n",
    "print(f\"Форма массива индексов: {I.shape}\")\n",
    "print(\"Начинаем Переранжировку и Агрегацию (для submission.csv)...\")\n",
    "\n",
    "# Получаем массивы с данными (чанков)\n",
    "chunk_web_ids_array = chunks_df['web_id'].values\n",
    "chunk_texts_array = chunks_df['text'].values\n",
    "\n",
    "# --- ИЗМЕНЕНИЯ ЗДЕСЬ ---\n",
    "# Берем данные вопросов 'to_process'\n",
    "q_ids_array = questions_to_process['q_id'].values\n",
    "query_texts_list = questions_to_process['query'].tolist()\n",
    "# ---\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for i in range(len(q_ids_array)):\n",
    "    q_id = q_ids_array[i]\n",
    "    query_text = query_texts_list[i]\n",
    "    \n",
    "    top_chunk_indices = I[i]\n",
    "    top_chunks_texts = chunk_texts_array[top_chunk_indices]\n",
    "    top_chunks_web_ids = chunk_web_ids_array[top_chunk_indices]\n",
    "\n",
    "    cross_inp = [[query_text, chunk_text] for chunk_text in top_chunks_texts]\n",
    "    cross_scores = cross_encoder.predict(cross_inp, show_progress_bar=False)\n",
    "    \n",
    "    web_id_scores = {}\n",
    "    reranked_data = list(zip(cross_scores, top_chunks_web_ids))\n",
    "    \n",
    "    for score, web_id in reranked_data:\n",
    "        if web_id not in web_id_scores:\n",
    "            web_id_scores[web_id] = 0.0\n",
    "        web_id_scores[web_id] += score\n",
    "        \n",
    "    sorted_web_ids = sorted(web_id_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_5_web_ids = [web_id for web_id, score in sorted_web_ids[:5]]\n",
    "    \n",
    "    for web_id in top_5_web_ids:\n",
    "        results_list.append({'q_id': q_id, 'web_id': web_id})\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Обработано {i+1} / {len(q_ids_array)} вопросов для submission...\")\n",
    "\n",
    "# Создаем финальный DataFrame\n",
    "submit_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Сохраняем в CSV\n",
    "# Имя файла 'submission.csv' соответствует требованиям\n",
    "submit_df.to_csv('submission.csv', index=False) \n",
    "\n",
    "print(f\"\\nФайл submission.csv (с {len(q_ids_array)} вопросами) успешно создан!\")\n",
    "print(submit_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f60643-0bd3-4f08-920d-f6b9c271cbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8361c5f-f6f7-4715-bdcd-971c84b7be76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
